# 感知机

感知机是一种二类分类的模型，其输入为实例的特征向量，输出为实例的类别，即：在二分类问题中，其输出应为+1与-1。感知机将会作用于输入空间，将其分为划分为正负两类的分离超平面，其任务即在于求出将训练模型进行线性划分的超平面，为此，导入基于误分类的损失函数，利用梯度算法将其最小化即可求得感知机模型。

## 基本形式

不妨假设输入空间为$X \subseteq  \mathbb R^n $，输出空间为$Y=\{+1,-1\}$。输入 $x  \subseteq  X $表示实例的特征向量，对于输入空间的点，输出$y \subseteq Y$表示实例的类别，即可以得出以下函数：
$$
f(x)=sign(\omega \cdot x+b)
$$
称为感知机，其中的$\omega$​与$b$​即为感知机的模型参数，$\omega \subseteq  \mathbb R^n $​即为权值或权值向量,$b\subseteq R $​​作为偏置​，$sign$​​为符号函数。

感知机作为一种线性分类模型，属于判别模型；其几何解释为：

线性方程:
$$
\omega \cdot x+b=0
$$
对应于特征空间$ \mathbb R^n $​中的一个超平面$S$，其中$\omega$​对应为该超平面的法向量，而$b$为此超平面的截距。它可以将整个特征空间划分为两个部分，其两侧的点分别为正负两类，因此，该超平面也被称为**分离超平面**。

## 学习策略

### 数据集线性可分性

给定一个数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots (x_n,y_n)\}$，其中$x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N$​，如果存在某一个超平面，可以通过

$$
\omega \cdot x+b=0
$$
将数据集的正负实例点完全正确的划分到两侧，则称该数据集$T$​​为**线性可分数据集**。

**应当注意，感知机只可以应用于线性可分集中。**

### 学习策略

在这一算法中，为了找出可以完全划分的超平面，可以将误分类点到超平面S的总距离作为损失函数进行优化，可以写出任意一点到超平面的距离为：
$$
S=\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|
$$
其中$\|w\|$即为$w$的$L_2$​​系数。

而由误分类点的性质可知，其距离可以被定义为：
$$
S=-\frac{1}{\|w\|}y_i(w \cdot x_{i}+b)
$$
基于此，可知求和结果为：
$$
-\frac{1}{\|w\|} \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$
其中的M即为该超平面误分类的集合，而若不考虑$L_2$系数，即可以写出学习的损失函数如下：
$$
L(w,b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$
而针对其优化过程，**采取的是随机梯度下降法，即首先初始化某一个超平面$w_0,b_0$，然后使用梯度下降法不断地极小化目标函数，且在这一过程中，每一次只是选取一个误分类点进行梯度下降法计算。**

在这一过程中，可以写出：
$$
\nabla_{w} L(w, b)=-\sum_{x_{i} \in M} y_{i} x_{i}
$$

$$
\nabla_{b} L(w, b)=-\sum_{x_{i} \in M} y_{i}
$$

在这一情况下，可以设置学习率为$\lambda$,其更新过程为，随机选取一个误分类点$(x_i,y_i)$后进行更新：
$$
w \leftarrow w+\lambda y_ix_i \qquad b \leftarrow w+\lambda y_i
$$

### 算法收敛性

可以证明，在经过有限次迭代后，一定可以找到一个满足条件的超平面：

不妨将偏置系数$b$并入$w$并将其写为$\hat{w}$，即$\hat x=(w^T,b)^T$，同样的对$x$进行扩充,加入常数1并写为$\hat x$，即可知$\hat w \cdot \hat x=w\cdot x +b$，由此可以写出`Novikoff`定理

设训练数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$​ 是线性可分的，其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, \quad y_{i} \in \mathcal{Y}=\{-1,+1\}, \quad i=1,2, \cdots, N$​, 则
（1）存在满足条件 $\left\|\hat{w}_{\text {opt }}\right\|=1$​ 的超平面 $\hat{w}_{\text {opt }} \cdot \hat{x}=w_{\text {opt }} \cdot x+b_{\text {opt }}=0$​ 将训练数据集完 全正确分开；且存在 $\gamma>0$​, 对所有 $i=1,2, \cdots, N$​
$$
y_{i}\left(\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma
$$
（2） 令 $R=\max _{1 \leqslant i \leqslant N}\left\|\hat{x}_{i}\right\|$, 则感知机算法 $2.1$ 在训练数据集上的误分类次数 $k$ 满足 不等式
$$
k \leqslant\left(\frac{R}{\gamma}\right)^{2}
$$
以上定理的证明过程不太重要，但可以说明：当训练数据集线性可分时，感知机学习算法原始形式的迭代是收敛的，党说，其仍存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序，为了得到唯一超平面，可以选择在其基础上增加约束条件，这也是SVM的基本思想。

### 对偶形式

对偶形式的基本想法是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$。

其具体实现过程在于：

假设初始值$w_0$,$b_0$均为0。对误分类点$（x_i，y_i）$通过

公式
$$
w \leftarrow w+\lambda y_ix_i \qquad b \leftarrow w+\lambda y_i
$$
逐步修改$w,b$​得到最终结果，可以将这一过程视为正向对参数进行修改而非反向进行：



输入：线性可分的数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$​ 是线性可分的，其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, \quad y_{i} \in \mathcal{Y}=\{-1,+1\}, \quad i=1,2, \cdots, N$​​, 学习率为$\eta \in(0,1) $​​：

输出: $\alpha, b ;$​ 感知机模型 $f(x)=\operatorname{sign}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x+b\right)$​，其中 $\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}} .$

1. 
      $\alpha \leftarrow 0, \quad b \leftarrow 0$​​
2.   在训练集中选取数据 $\left(x_{i}, y_{i}\right)$​
3. 如果 $y_{i}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0$​
     $\alpha_{1} \leftarrow \alpha_{i}+\eta$​
     $b \leftarrow b+\eta y_{i}$​
4.  转至(2) 直到没有误分类数据

### 总结

感知机是根据输入实例的特征向量$x$​对其进行二类分类的线性分类模型，其策略为最小化距离函数，其计算过程基于随机梯度下降法，每次选择一个误分类节点进行计算直到使得所有点都被正确分类，这一过程一定可以在有限次中完成。

