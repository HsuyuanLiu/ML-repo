# Boosting方法

提升（boosting）方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。

### Adaboost方法

提升方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。**即可以通过将多个弱学习器进行集合，可以达到超过某一强学习器的效果。**

提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。**大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。**

在具体实现的过程中，`adaboos`t方法将会提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。在最后，对不同的学习器进行分类表决，得到最终的强学习结果

#### 实现

不妨给定一个二类分类的数据集$T={(x_1,y_1),(x_2,y_2)...(x_N,y_N)}$，其中每个样本点由实例与标记组成。实例$x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N$​，则其方法如下：

1. 初始化训练数据的权值分布

$$
D_1=(w_{11},...,w_{1i},...,w_{1N})  \qquad w_{1i}=\frac{1}{N}, \qquad i=1,2...N
$$

2. 对于M（M为弱分类器的个数）进行学习，得到基本分类器:
$$
G_m(x):\mathcal X \rightarrow \{-1,1\}
$$
然后计算该分类器在数据集上的分类误差率$e_m$

计算$G_m(x)$​的系数：
$$
\alpha_m=\frac{1}{2}\log \frac{1-e_m}{e_m}
$$
最终更新训练数据集的权重分布：
$$
\begin{gathered}
D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})  \\
w_{m+1,i}=\frac{w_{m,i}}{Z_m}\exp(-\alpha_my_iG_m(x_i))
\end{gathered}
$$
在这一步中，被上一个分类器错误分类的样本将会被增加权重，$Z_m$为规范化因子。，使得$D_{m+1}$成为一个分布。

3. 构建基本分类器的组合得到最终分类器：

$$
f(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$

在步骤一中假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$​；

在步骤二中，

- 使用当前分布$D_m$​加权的训练数据集，学习基本分类器$G_m(x)$
- 计算基本分类器$G_m(x$)在加权训练数据集上的分类误差率：
- 计算基本分类器$G_m(x)$的系数$\alpha_m$
- 更新训练数据的权值分布为下一轮作准备。

在步骤三中，

线性组合$f(x$)实现M个基本分类器的加权表决。系数$\alpha_m$表示了基本分类器$G_m(x)$的重要性

#### 误差分析

AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。可以证明，其最终的训练误差，满足:
$$
\frac{1}{N} \sum_{i=1}^NI(G(x_i)\neq y_1 ) \leq \frac{1}{N} \sum_{i=1}^N \exp(-y_if(x_i))=\prod_m Z_m
$$
这一定理说明，可以在每一轮选取适当的$G_m$​使得$Z_m$​最小，从而使训练误差下降最快。

#### 另一种解释

**AdaBoost算法还有另一个解释，即可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法。**

鉴于整个模型可以看成若干个模型的加权，前向分步算法求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：
$$
\min_{\beta,\gamma} \sum^N_{i=1}L(Y_I,\beta b(x_i;\gamma_i))
$$
基于此，其方法如下：

设其损失函数为$L(Y,f(x))$​，基函数集合为$\{b(x;\gamma )\}$​，加法模型 $\mathrm{f}(\mathrm{x})_{}$；​

1. 初始化 $\mathrm{f}_{0}(\mathrm{x})=0$​
2. 对 $\mathrm{m}=1,2, \ldots, \mathrm{M}$​​
极小化损失函数

$$
\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, y} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)
$$
​	得到参数 $\beta_{m}, \gamma_{m}$​，并对损失函数进行更新：
$$
f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)
$$

3. 得到加法模型：
   $$
   f(x)=f_M(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
   $$
   

这样，前向分步算法将同时求解从$m＝1$到$M$所有参数$\beta_m$，$\gamma_m$的优化问题简化为逐次求解各个$\beta_m$，$\gamma_m$的优化问题

### Boosting Tree

提升树是以分类树或回归树为基本分类器的提升方法，被认为是统计学习中性能最好的方法之一。

提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。之前中看到的基本分类器$x<v$或$x>v$​，可以看作是由一个根结点直接连接两个叶结点的简单决策树，即所谓的决策树桩；提升树模型可以表示为决策树的加法模型：
$$
f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
$$

其中，$T(x;\Theta_m)$​表示决策树，$M$为决策树的个数，$\Theta$表示其中的参数。​

##### 算法

**提升树算法采用前向分步算法**。首先确定初始提升树$f_0(x)＝0$，第$m$歩的模型是
$$
f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right)
$$
其中， $\mathrm{f}_{\mathrm{m}-1}(\mathrm{x})$ 为当前模型，通过经验风险极小化确定下一棵决策树的参数 $\Theta_{\mathrm{m}}$,
$$
\hat{\Theta}_{m}=\arg \min _{\Theta_{m}} \sum_{i}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \Theta_{m}\right)\right)
$$
在这一情况下，对于不同的损失函数将使用不同的算法。

