{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "train_data=pd.read_csv(\"test_data.csv\")\n",
    "#print(train_data)\n",
    "train_data=train_data.T\n",
    "train_data.to_csv(\"testT_data.csv\")\n",
    "data=train_data.values\n",
    "data_values=data.T  #进行转置？\n",
    "#print(train_data)\n",
    "adata=sc.read_csv(\"testT_data.csv\")#应该进行装置\n",
    "adata=adata[1:201]\n",
    "print(type(data))\n",
    "a,gene_num=adata.shape\n",
    "sc.pp.filter_cells(adata, min_counts=1)\n",
    "adata.raw = adata.copy()\n",
    "adata.obs['size_factors'] = adata.obs.n_counts / np.median(adata.obs.n_counts)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.scale(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集组成为 200个细胞，每个细胞1000个基因需要进行表达"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编码方式\n",
    "class seq_dataset(Dataset):\n",
    "    def __init__(self,load_data):\n",
    "        self.data = load_data\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    def __len__(self,):\n",
    "        return len(self.data)\n",
    "RNA=seq_dataset(data_values)\n",
    "train_RNA_dataloader=DataLoader(RNA,batch_size=32,shuffle=True)\n",
    "count_mtx = torch.tensor(adata.X)\n",
    "count_mtx_raw = torch.tensor(adata.raw.X)\n",
    "size_factor = torch.tensor(adata.obs.size_factors.values)\n",
    "dataset = TensorDataset(count_mtx, size_factor,count_mtx_raw)\n",
    "#print(dataset)\n",
    "train1_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for item in train1_dataloader:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题在于：对true data的使用以及在这一过程中对dataset的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(mat):\n",
    "    if type(mat) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(mat.weight,gain=3)\n",
    "        print(\"Weight:\",mat.weight)\n",
    "        mat.bias.data.fill_(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mean_activation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mean_activation,self).__init__()\n",
    "    def forward(self,input):\n",
    "        return torch.exp(input)\n",
    "\n",
    "class dispersion_activiation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dispersion_activiation,self).__init__()\n",
    "    def forward(self,input):\n",
    "        return torch.exp(input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class auto_encoder(nn.Module):\n",
    "    def __init__( self,gene_num,hidden_dim):\n",
    "        super(auto_encoder,self).__init__()\n",
    "        self.gene_num = gene_num\n",
    "        print(\"gene_num: \" ,gene_num)\n",
    "\n",
    "        self.autoencoder_stack=nn.Sequential(\n",
    "            nn.Linear(gene_num,hidden_dim[0]),\n",
    "            nn.BatchNorm1d(hidden_dim[0],eps=1e-2,affine=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim[0],hidden_dim[1]),\n",
    "            nn.BatchNorm1d(hidden_dim[1],eps=1e-2,affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim[1],hidden_dim[2]),\n",
    "            nn.BatchNorm1d(hidden_dim[2],eps=1e-2,affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mean=nn.Sequential(\n",
    "            nn.Linear(hidden_dim[2],gene_num),\n",
    "            mean_activation()\n",
    "        )\n",
    "        self.pi=nn.Sequential(\n",
    "            nn.Linear(hidden_dim[2],gene_num),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.theta=nn.Sequential(\n",
    "            nn.Linear(hidden_dim[2],gene_num),\n",
    "            dispersion_activiation()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, size_factor):\n",
    "        size_factor = size_factor.view(-1, 1)#?\n",
    "        size_factor = size_factor.repeat(1, self.gene_num)\n",
    "        #ae_out = input * size_factor\n",
    "        ae_out = self.autoencoder_stack(input) #?\n",
    "        #print(\"AE:  \", ae_out)\n",
    "        mean = self.mean(ae_out) * size_factor\n",
    "        #print(\"MEAN:  \", mean)\n",
    "        theta = self.theta(ae_out)\n",
    "        pi = self.pi(ae_out)\n",
    "        return mean, theta, pi\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nan2inf(x):\n",
    "    return torch.where(torch.isnan(x), torch.zeros_like(x) + np.inf, x)\n",
    "class NB(torch.nn.Module):\n",
    "    def __init__(self, scale_factor=1.0):\n",
    "        super(NB, self).__init__()\n",
    "        self.eps = 1e-10\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, theta, y_true, y_pred, mean=True):\n",
    "        y_pred = y_pred * self.scale_factor\n",
    "        t1 = torch.lgamma(theta + self.eps) + torch.lgamma(y_true + 1.0) - torch.lgamma(y_true + theta + self.eps)\n",
    "        t2 = (theta + y_true) * torch.log(1.0 + (y_pred / (theta + self.eps))) + (\n",
    "                y_true * (torch.log(theta + self.eps) - torch.log(y_pred + self.eps)))\n",
    "\n",
    "        final = t1 + t2\n",
    "        final = _nan2inf(final)\n",
    "\n",
    "        if mean:\n",
    "            final = torch.mean(final)\n",
    "\n",
    "        return final\n",
    "class ZINB(NB):\n",
    "    def __init__(self, scale_factor=1.0, ridge_lambda=0.0):\n",
    "        super().__init__(scale_factor=scale_factor)\n",
    "        self.ridge_lambda = ridge_lambda\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, pi, theta, y_true, y_pred, mean=True):\n",
    "        nb_case = super().forward(theta, y_true, y_pred, mean=False) - torch.log(1.0 - pi + self.eps)\n",
    "\n",
    "        y_pred = y_pred * self.scale_factor\n",
    "        # theta = torch.minimum(theta, torch.tensor(1e6))\n",
    "\n",
    "        zero_nb = torch.pow(theta / (theta + y_pred + self.eps), theta)\n",
    "        zero_case = -torch.log(pi + ((1.0 - pi) * zero_nb) + self.eps)\n",
    "        result = torch.where(torch.lt(y_true, 1e-8), zero_case, nb_case)\n",
    "\n",
    "        if self.ridge_lambda > 0:\n",
    "            ridge = self.ridge_lambda * torch.square(pi)\n",
    "            result += ridge\n",
    "\n",
    "        if mean:\n",
    "            result = torch.mean(result)\n",
    "\n",
    "        result = _nan2inf(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 180\n",
    "grad_clip_val = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene_num:  1000\n",
      "Weight: Parameter containing:\n",
      "tensor([[ 0.0156, -0.1413,  0.0533,  ...,  0.0188, -0.1021,  0.1449],\n",
      "        [-0.1138, -0.1066, -0.1866,  ...,  0.1941, -0.0847,  0.1680],\n",
      "        [ 0.1528, -0.1063,  0.2082,  ...,  0.2057,  0.0848, -0.1067],\n",
      "        ...,\n",
      "        [-0.0009, -0.1513,  0.0012,  ...,  0.1231,  0.1540,  0.1538],\n",
      "        [ 0.0930, -0.1259, -0.1278,  ..., -0.1696,  0.1224,  0.2214],\n",
      "        [ 0.0305,  0.1558, -0.0392,  ..., -0.1160,  0.1739,  0.2039]],\n",
      "       requires_grad=True)\n",
      "Weight: Parameter containing:\n",
      "tensor([[-0.6372,  0.2679, -0.0640,  ..., -0.5816,  0.2378, -0.3739],\n",
      "        [-0.2608, -0.1821, -0.6581,  ...,  0.1831,  0.2766, -0.4294],\n",
      "        [ 0.5798, -0.4580, -0.6823,  ..., -0.1803, -0.5637, -0.3464],\n",
      "        ...,\n",
      "        [ 0.5452,  0.5915, -0.6691,  ...,  0.4414,  0.2958,  0.4173],\n",
      "        [-0.5428,  0.2755,  0.4818,  ..., -0.4206,  0.6115, -0.5740],\n",
      "        [-0.0534, -0.1294,  0.2222,  ..., -0.2887,  0.4565, -0.3400]],\n",
      "       requires_grad=True)\n",
      "Weight: Parameter containing:\n",
      "tensor([[ 0.6972,  0.1386, -0.4058,  ..., -0.2748, -0.4136, -0.3395],\n",
      "        [-0.7282,  0.6978, -0.0981,  ...,  0.6888,  0.6788, -0.5746],\n",
      "        [ 0.3936, -0.5183, -0.2095,  ...,  0.0074, -0.6736, -0.2958],\n",
      "        ...,\n",
      "        [-0.6610, -0.6217, -0.2124,  ...,  0.4136,  0.5474,  0.5561],\n",
      "        [ 0.0933,  0.4249,  0.6307,  ...,  0.5464,  0.3786, -0.7496],\n",
      "        [-0.5189, -0.5835,  0.1179,  ..., -0.4558, -0.7400,  0.4240]],\n",
      "       requires_grad=True)\n",
      "Weight: Parameter containing:\n",
      "tensor([[-0.2230,  0.0244,  0.1054,  ..., -0.0155, -0.0883,  0.0500],\n",
      "        [ 0.1805,  0.1183, -0.1962,  ...,  0.0726,  0.1317,  0.1668],\n",
      "        [-0.1303, -0.0209,  0.1763,  ...,  0.1548, -0.1611, -0.1325],\n",
      "        ...,\n",
      "        [ 0.2206,  0.0599, -0.1450,  ..., -0.1874,  0.1062,  0.0446],\n",
      "        [ 0.1524, -0.0604,  0.0489,  ...,  0.1078, -0.1295, -0.0897],\n",
      "        [-0.1650,  0.1235, -0.1200,  ..., -0.1470,  0.0754,  0.1007]],\n",
      "       requires_grad=True)\n",
      "Weight: Parameter containing:\n",
      "tensor([[-0.0052, -0.2238,  0.0694,  ..., -0.0378, -0.0453, -0.0097],\n",
      "        [ 0.1743,  0.2039, -0.0751,  ...,  0.0919, -0.1457,  0.1912],\n",
      "        [ 0.2132,  0.0706, -0.1950,  ..., -0.0414, -0.1244, -0.0271],\n",
      "        ...,\n",
      "        [-0.1982, -0.1995,  0.0972,  ...,  0.1702,  0.0740, -0.0082],\n",
      "        [ 0.0729, -0.1201,  0.1909,  ..., -0.1834, -0.1979, -0.0493],\n",
      "        [ 0.1199, -0.0905,  0.1014,  ..., -0.2178,  0.1129,  0.1615]],\n",
      "       requires_grad=True)\n",
      "Weight: Parameter containing:\n",
      "tensor([[-0.1818, -0.1666,  0.0155,  ...,  0.0677, -0.0796,  0.0768],\n",
      "        [-0.1941,  0.0573,  0.0956,  ...,  0.2099, -0.0396,  0.1571],\n",
      "        [ 0.0051,  0.0742,  0.1506,  ...,  0.0607, -0.1715, -0.0178],\n",
      "        ...,\n",
      "        [-0.0919, -0.0583, -0.1469,  ..., -0.0270,  0.0813, -0.0204],\n",
      "        [ 0.0662,  0.1550, -0.0707,  ..., -0.1487, -0.0954,  0.0121],\n",
      "        [ 0.1701, -0.0883,  0.0157,  ..., -0.0831, -0.0730,  0.1827]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "auto_encoder(\n",
       "  (autoencoder_stack): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=0.01, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): BatchNorm1d(32, eps=0.01, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (7): BatchNorm1d(64, eps=0.01, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (mean): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1000, bias=True)\n",
       "    (1): mean_activation()\n",
       "  )\n",
       "  (pi): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1000, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (theta): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1000, bias=True)\n",
       "    (1): dispersion_activiation()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#gene_num=count_mtx_raw.shape[1]\n",
    "model = auto_encoder(gene_num=gene_num, hidden_dim=(64, 32, 64))\n",
    "model.apply(weights_init)#不完整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = ZINB()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, epochs / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight init看着没有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,model,loss,optimizer):\n",
    "    #size_data=len(dataloader.dataset)\n",
    "    for batch, (X, size, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "    # Predict, and calculate loss\n",
    "        mean, theta, pi = model(y, size)\n",
    "        #loss_val = loss(mean, theta, pi, y)\n",
    "        loss_val= loss (pi,theta,y,mean,mean=True)\n",
    "        loss_val.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip_val)\n",
    "        optimizer.step()\n",
    "        if batch % 50 == 0:\n",
    "            #loss, curr = loss_val, batch * len(X)\n",
    "            print(f\"loss: {loss_val}\")\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "loss: 48.6239013671875\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 13.85482120513916\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 7.729384422302246\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 6.316256523132324\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 5.631008625030518\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 5.1686224937438965\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.844146251678467\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 4.6106390953063965\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 4.528166770935059\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 4.542829513549805\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 4.324438571929932\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 4.200567722320557\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 4.241685390472412\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 4.083768367767334\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 4.108485698699951\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 4.014965057373047\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 3.983633041381836\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 3.9002816677093506\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 3.860685110092163\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 3.8557727336883545\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 3.785635232925415\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 3.8034491539001465\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 3.8334546089172363\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 3.773794651031494\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 3.7742254734039307\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 3.6765944957733154\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 3.7103750705718994\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 3.738381862640381\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 3.632422924041748\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 3.6354682445526123\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 3.5826971530914307\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 3.5712153911590576\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 3.609285831451416\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 3.5702993869781494\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 3.5155298709869385\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 3.5552561283111572\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 3.5890634059906006\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 3.4916884899139404\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 3.4546852111816406\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 3.4667043685913086\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 3.5260653495788574\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 3.434297800064087\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 3.478351593017578\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 3.4625988006591797\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 3.447054862976074\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 3.4957845211029053\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 3.469683885574341\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 3.4555139541625977\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 3.3963942527770996\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 3.438793897628784\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 3.3926150798797607\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 3.3851964473724365\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 3.3487696647644043\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 3.3373665809631348\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 3.345062494277954\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 3.318681240081787\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 3.3326029777526855\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 3.2985401153564453\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 3.300731658935547\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 3.3184120655059814\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 3.3546030521392822\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 3.263866662979126\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 3.323258399963379\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 3.2705130577087402\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 3.223473310470581\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 3.390122413635254\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 3.3104171752929688\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 3.2279715538024902\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 3.225172758102417\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 3.2708542346954346\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 3.2371864318847656\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 3.224423885345459\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 3.243010997772217\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 3.266911268234253\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 3.2764689922332764\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 3.202749490737915\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 3.353187084197998\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 3.171518564224243\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 3.222186326980591\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 3.197071075439453\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 3.1625096797943115\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 3.1529929637908936\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 3.1330180168151855\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 3.155325174331665\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 3.1103320121765137\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 3.150700330734253\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 3.128324031829834\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 3.135462999343872\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 3.1195409297943115\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 3.123973846435547\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 3.211566209793091\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 3.1514179706573486\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 3.142592430114746\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 3.0940113067626953\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 3.184664249420166\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 3.0898380279541016\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 3.0919699668884277\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 3.177642583847046\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 3.0915112495422363\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 3.082406520843506\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 3.1200926303863525\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 3.0518553256988525\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 3.0891504287719727\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 3.124514579772949\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 3.062844753265381\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 3.1494548320770264\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 3.093977689743042\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 3.1124982833862305\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 3.069871425628662\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 3.1719677448272705\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 3.1268980503082275\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 3.115659713745117\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 3.0515244007110596\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 3.05464506149292\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 3.173964738845825\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 3.0018913745880127\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 3.0586984157562256\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 3.0831520557403564\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 3.076047658920288\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 3.02239727973938\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 3.015852689743042\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 3.07908034324646\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 3.033501625061035\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 2.993842601776123\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 3.093519687652588\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 3.070512056350708\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 3.03266978263855\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 3.0212419033050537\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 3.051659345626831\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 3.0876364707946777\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 3.0285167694091797\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 3.0089306831359863\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 3.0616374015808105\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 3.0720200538635254\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 3.038996934890747\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 3.0160138607025146\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 3.049426555633545\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 3.0188212394714355\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 3.0147078037261963\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 3.0056896209716797\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 2.978803873062134\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 3.013554334640503\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 3.032771587371826\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 2.9852023124694824\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 3.0030221939086914\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 2.9796152114868164\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 3.0362439155578613\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 2.9974143505096436\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 2.9963393211364746\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 3.0334789752960205\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 3.003828763961792\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 3.0113821029663086\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 3.000502347946167\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 3.0105433464050293\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 2.9839441776275635\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 3.0831289291381836\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 3.0103704929351807\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 3.00040602684021\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 2.986682653427124\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 2.9717183113098145\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 2.981196880340576\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 3.012866497039795\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 2.940173625946045\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 2.995018482208252\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 3.158651113510132\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 3.0010502338409424\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 3.020596742630005\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 3.0097525119781494\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 2.965651035308838\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 2.942307949066162\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 2.9794371128082275\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 3.018749952316284\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 3.0104329586029053\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 2.9630720615386963\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 2.9503328800201416\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 3.05320405960083\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 2.954758882522583\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 2.9647178649902344\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 2.9793989658355713\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 2.972045660018921\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 2.9730093479156494\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 2.9823594093322754\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 3.004817485809326\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 2.9768922328948975\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 2.9318060874938965\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 3.0231380462646484\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 3.0402920246124268\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 2.992079973220825\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 3.0018882751464844\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 2.9438729286193848\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 2.982243061065674\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 3.011185646057129\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 2.9623665809631348\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 2.9866366386413574\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 3.011019229888916\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 2.9592716693878174\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 2.9847402572631836\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 2.987069606781006\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 2.959947109222412\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 2.9674198627471924\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 2.9734160900115967\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 2.9491045475006104\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 3.0048625469207764\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 2.9915802478790283\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 2.956045150756836\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 2.9818763732910156\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 3.0074222087860107\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 2.9449703693389893\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 2.9701836109161377\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 2.9588334560394287\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 2.9541409015655518\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 2.94805645942688\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 2.9473423957824707\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 2.9927308559417725\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 2.9985053539276123\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 3.012444019317627\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 2.966154098510742\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 2.947783946990967\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 2.9470789432525635\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 3.027885913848877\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 2.99568510055542\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 2.9945313930511475\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 2.963486671447754\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 2.9651241302490234\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 2.9998114109039307\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 2.992978096008301\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 2.9991977214813232\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 3.016585350036621\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 2.9631383419036865\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 2.9897353649139404\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 2.94228458404541\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 2.9611215591430664\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 2.9645323753356934\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 3.019021511077881\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 2.9365525245666504\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 2.941739797592163\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 2.9675991535186768\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 3.058906078338623\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 2.9720187187194824\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 2.960312604904175\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 2.943380832672119\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 2.9678213596343994\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 2.993889093399048\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 2.970221996307373\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 2.9804797172546387\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 2.9473118782043457\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 2.9439315795898438\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 2.979228973388672\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 2.97831130027771\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 3.029383659362793\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 2.9312078952789307\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 2.9581708908081055\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 2.9379396438598633\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 2.9680538177490234\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 3.0069632530212402\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 2.9906909465789795\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 2.981334924697876\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 2.9414496421813965\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 2.9850449562072754\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 2.9718570709228516\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 2.9799046516418457\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 2.9364311695098877\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 2.9767699241638184\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 2.949580669403076\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 3.0066492557525635\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 2.9475960731506348\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 2.9615468978881836\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 2.922069549560547\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 2.9340145587921143\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 2.963134288787842\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 2.994664192199707\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 2.9892191886901855\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 2.9899697303771973\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 2.975027561187744\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 2.932494640350342\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 2.9571640491485596\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 3.0016167163848877\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 2.988111734390259\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 2.9318246841430664\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 2.9139764308929443\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 2.954627275466919\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 3.008052110671997\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 3.022887706756592\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 3.0034916400909424\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 2.9237277507781982\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 2.9875435829162598\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 2.950880527496338\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 3.01579213142395\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 2.955271005630493\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 2.9167535305023193\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 2.9378414154052734\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 2.9723939895629883\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 2.9567995071411133\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 2.940716505050659\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 2.9498119354248047\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 2.996760845184326\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 3.028862714767456\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 2.9667398929595947\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 2.994403839111328\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 2.963428497314453\n"
     ]
    }
   ],
   "source": [
    "for iter in np.arange(epochs):\n",
    "    print(f\"Epoch {iter}\\n-------------------------------\")\n",
    "    train(train1_dataloader,model,loss_fn,optimizer)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.3947e+00, 4.9294e+02, 3.6475e+01,  ..., 1.0418e-01, 1.1592e+00,\n",
      "         4.1442e+01],\n",
      "        [4.7040e+00, 6.9294e+02, 5.2233e+01,  ..., 6.5395e-02, 1.6272e+00,\n",
      "         4.7663e+01],\n",
      "        [3.1404e+00, 4.1121e+02, 3.0450e+01,  ..., 5.7691e-02, 9.4699e-01,\n",
      "         3.0292e+01],\n",
      "        ...,\n",
      "        [4.6547e+00, 4.9638e+02, 4.6179e+01,  ..., 1.0357e-01, 1.5509e+00,\n",
      "         7.1354e+01],\n",
      "        [4.4502e+00, 7.4635e+02, 5.0571e+01,  ..., 5.9038e-02, 1.6093e+00,\n",
      "         5.8876e+01],\n",
      "        [3.8597e+00, 5.8771e+02, 4.3817e+01,  ..., 1.9420e-01, 1.2540e+00,\n",
      "         2.0852e+01]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean, theta, pi = model(count_mtx_raw, size_factor)\n",
    "print(mean)\n",
    "mean_value=mean.detach().numpy()\n",
    "#print(theta)\n",
    "#print(pi)\n",
    "#print(mean.shape,train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def impute_zero(mean, previous_data):\n",
    "    imputed=previous_data.copy()\n",
    "    print(imputed)\n",
    "    for item in np.arange(imputed.shape[0]):\n",
    "        for ele in np.arange(imputed.shape[1]):\n",
    "            if imputed[item][ele]==0:\n",
    "                imputed[item][ele]=mean[item][ele]\n",
    "    return imputed\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 542  48 ...   0   0  32]\n",
      " [  7 772  63 ...   0   0  62]\n",
      " [  0 449  29 ...   0   0  20]\n",
      " ...\n",
      " [  1 513  24 ...   0   1  38]\n",
      " [  0 786  32 ...   0   0  73]\n",
      " [  0 640  98 ...   0   3  20]]\n",
      "[[  3 542  48 ...   0   1  32]\n",
      " [  7 772  63 ...   0   1  62]\n",
      " [  3 449  29 ...   0   0  20]\n",
      " ...\n",
      " [  1 513  24 ...   0   1  38]\n",
      " [  4 786  32 ...   0   1  73]\n",
      " [  3 640  98 ...   0   3  20]]\n"
     ]
    }
   ],
   "source": [
    "imputed_res=impute_zero(mean,data)\n",
    "print(imputed_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1000) (200, 1000)\n",
      "Dropout Previous : 0.236915\n",
      "Dropout Imputed : 0.060635\n",
      "Imputed Value Sum: 99373.0\n",
      "Previous Value Sum:  157508\n"
     ]
    }
   ],
   "source": [
    "def previous_diff(previous,file_path):\n",
    "    true_data=pd.read_csv(file_path)\n",
    "    true_data=np.array(true_data)\n",
    "    true_data=true_data.T\n",
    "    dist=0\n",
    "    for item in np.arange(previous.shape[0]):\n",
    "        for ele in np.arange(previous.shape[1]):\n",
    "            if previous[item][ele]==0:\n",
    "                res=true_data[item][ele]\n",
    "                dist=dist+res\n",
    "    print(\"Previous Value Sum: \",dist)\n",
    "def calculate_distance(mat,file_path,imputed):\n",
    "    distance = 0;\n",
    "    count_pre,count_after=0,0\n",
    "    true_data=pd.read_csv(file_path)\n",
    "    true_data=np.array(true_data)\n",
    "    true_data=true_data.T\n",
    "    print(data.shape,true_data.shape)\n",
    "    for item in np.arange(mat.shape[0]):\n",
    "        for ele in np.arange(mat.shape[1]):\n",
    "            if mat[item][ele]==0:\n",
    "                count_pre+=1\n",
    "                if np.round(imputed[item][ele])==0:\n",
    "                    count_after+=1\n",
    "                res=np.round(imputed[item][ele]-true_data[item][ele])\n",
    "                #print(mat[item][ele]-true_data[item][ele])\n",
    "                distance+=np.abs(res)\n",
    "    print(\"Dropout Previous :\",count_pre/(mat.shape[0]*mat.shape[1]))\n",
    "    print(\"Dropout Imputed :\",count_after/(mat.shape[0]*mat.shape[1]))\n",
    "    print(\"Imputed Value Sum:\",distance)\n",
    "    return distance\n",
    "dist=calculate_distance(data,\"test_truedata.csv\",mean_value)\n",
    "previous_diff(data,\"test_truedata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对大数据集进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "data_test=pd.read_csv(\"data.csv\")\n",
    "test_data=data_test.T\n",
    "test_data.to_csv(\"data4test.csv\")\n",
    "finaldata=test_data.values\n",
    "data_values=finaldata.T  #进行转置？\n",
    "data2test=sc.read_csv(\"data4test.csv\")\n",
    "data2test=data2test[1:data2test.shape[0]]\n",
    "print(data2test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1000) (5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "sc.pp.filter_cells(data2test, min_counts=1)\n",
    "data2test.raw = data2test.copy()\n",
    "data2test.obs['size_factors'] = data2test.obs.n_counts / np.median(data2test.obs.n_counts)\n",
    "sc.pp.log1p(data2test)\n",
    "sc.pp.scale(data2test)\n",
    "count_mtx_raw_1 = torch.tensor(data2test.raw.X)\n",
    "size_factor_1 = torch.tensor(data2test.obs.size_factors.values)\n",
    "means, theta, pi = model(count_mtx_raw_1, size_factor_1)\n",
    "mean_value_final=means.detach().numpy()\n",
    "print(mean_value_final.shape,test_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   5 236 ...   4  31   6]\n",
      " [  0   0 313 ...   8  12   4]\n",
      " [  5  13 490 ...  13  22   6]\n",
      " ...\n",
      " [  3  10 245 ...  12  37   5]\n",
      " [  0   5 183 ...   7  14   0]\n",
      " [  4   1 481 ...  17  26  20]]\n"
     ]
    }
   ],
   "source": [
    "results=impute_zero(mean_value_final,finaldata)\n",
    "final_res=pd.DataFrame(results.T)\n",
    "final_res.to_csv(\"results.csv\",index=0,header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0240914\n"
     ]
    }
   ],
   "source": [
    "cnt=np.where(results,0,1)\n",
    "print(np.sum(cnt)/5000000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2345e6128660d800caeeaf1f3a6936fcd09960590a8a89cf9fb390fc735dae0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
