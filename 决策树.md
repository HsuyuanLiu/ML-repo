#  	 决策树

决策树（decision tree）是一种基本的分类与回归方法。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括3个步骤：**特征选择、决策树的生成和决策树的修剪。**

决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分（partition）上。将特征空间划分为互不相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布。假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$。决策树分类时将该结点的实例强行分到条件概率大的那一类去。

#### 学习过程

决策树学习，假设给定训练数据集
$$
D=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
$$
其中, ${x}_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}$​​​​ 为输入实例 (特征向量）, $\mathrm{n}$​​​​ 为特征个数, $\mathrm{y}_{\mathrm{i}} \in\{1,2, \ldots, \mathrm{K}\}$​​​​ 为类标记, $\mathrm{i}=1,2...\mathrm{N}$，$\mathrm{N}$​​​​ 为样本容量. 学习的目标是根据给定的训练数据集构建一个决策树模型，使它能对实例进行正确的分类。

**其本质上是从训练数据集中归纳出一组分类规则**。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多个。这一学习过程使用**正则化的极大似然函数**进行学习。其通过使得这一损失函数最小化的求解计算出结果。

从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优（sub-optimal）的。

在完成学习过程后，也需要进行对树的剪枝操作提高模型的泛化能力，对已生成的树自下而上进行剪枝，将树变得更简单。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。

## 特征选择

特征选择在于选取对训练数据具有分类能力的特征。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是**信息增益**或**信息增益比**，但应当注意，在基本情况下，某一特征只能被选择一次。

### 信息增益

首先给出熵与条件熵的定义：

熵（entropy）是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为
$$
P(X=x_i)=p_1,\qquad i=1,2,...,n
$$
该随机变量的熵定义为
$$
H(X)=-\sum_{i=1}^n p_ilogp_i
$$
可知$p＝0$​或$p＝1$时$H(p)＝0$，随机变量完全没有不确定性。当$p＝0.5$时，$H(p)＝1$，熵取值最大，随机变量不确定性最大。

而条件熵$H(X|Y)$可以同于表示已知随机变量X条件下随即变量Y的不确定性，随机变量X给定条件下Y的条件熵H(Y|X)定义为X给定条件下Y的条件分布概率分布的熵对X的数学期望
$$
H(Y|X)=-\sum_{i=1}^n p_iH(Y|X=x_i)
$$
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵，**信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。**			
基于此，即可以计算信息增益。					
				
**特征A对训练数据集D的信息增益$g(D,A)$​，定义为集合$D$的经验熵$H(D)$​与特征A给定条件下D的经验条件熵$H(D|A)$​之差，即**
$$
g(D,A)=H(D)-H(D|A)
$$
决策树学习应用信息增益准则选择特征。给定训练数据集D和特征A，经验熵$H(D)$表示对数据集D进行分类的不确定性。而经验条件熵$H(D|A)$表示在特征A给定的条件下对数据集D进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。显然，对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。

**在对特征进行选择的过程中，对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。**

基于此，其算法为：

设整个集合中共有K个类别$C_k$，$|C_k|$表示每一个类别中样本的个数，特征A有 $\mathrm{n}$ 个不同的取值 $\left\{\mathrm{a}_{1}, \mathrm{a}_{2}, \ldots, \mathrm{a}_{\mathrm{n}}\right\}$, 根据特征 $\mathrm{A}$ 的取值将D划分为 $\mathrm{n}$ 个子集$D_1,...D_n$ ，则
1. 计算数据集D的经验熵H(D)
$$
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$
2. 计算特征A对数据集D的经验条件熵 $\mathrm{H}(\mathrm{D} \mid \mathrm{A})$​
$$
H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
$$
3. 计算信息增益 $g(D, A)=H(D)-H(D \mid A)$​

除此之外，还可以使用信息增益比$\frac{g(D,A)}{H_A(D)}$​（$H_A(D)$​为训练数据集D关于特征A的值的熵）作为选择特征的依据。

### 剪枝

生成算法产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。因此需要某些方法对已生成的决策树进行简化。

**剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。**

决策树的剪枝往往通过极小化决策树整体的损失函数来实现。设叶结点个数为|T|，则对所有叶节点上的经验熵进行求和可以写出：
$$
C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
$$

$$
H_t(T)=-\sum_k \frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}
$$

其中，每一个叶节点都有$N_t$个叶节点且其中k类样本的点有$N_{tk}$个。

在以上公式中可知，

**$C(T)$​​表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$​​表示模型复杂度，参数$\alpha≥0$​​控制两者之间的影响。较大的$\alpha$​促使选择较简单的模型（树），较小的a促使选择较复杂的模型。$\alpha＝0$​​意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。**

而剪枝方法，在于：

- 计算每个结点的经验熵。

- 递归地从树的叶结点向上回缩

设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别是$C_\alpha(T_B)$与$C_\alpha(T_A)$，则若$C_\alpha(T_A) \le C_\alpha(T_B)$​，可以进行剪枝并将父节点作为新子节点

### CART

CART的机制在于**回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。**

#### 回归

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元$R_m$且每一个空间对应着一个固定的输出$c_m$。

即，可以写为如下形式：
$$
f(x)=\sum^M_{m=1}c_mI(x\in R_m)
$$
而在划分已经确定的情况下，可以使用平方误差来表示回归树对于训练数据的预测误差：
$$
min(\sum_{x_i \in R_m}(y_i-f(x_i))^2)
$$
而可知，在此情况下，$c_m$的最优值应为所有实例输出的平均值：
$$
c_m=average(y_i=f(x_i)|x_i \in R_m)
$$
而为了寻找切分点，这里采用启发式的方法，逐个遍历所有可能切分点，选取使得整体损失函数最小的点作为切分点：
$$
\min_{j,s}[\min_{c_1}(\sum_{x_i \in R_m(j,s)}(y_i-c_1)^2,\min_{c_2}(\sum_{x_i \in R_m(j,s)}(y_i-c_2)^2]
$$
此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。就可以生成一棵回归树。

#### 分类

CART中使用**基尼系数**作为信息增益对特征进行选择。

其表达式为：
$$
Gini(D,A)=1-\sum_{K=1}^K(\frac{|C_K|}{D})^2
$$
基尼指数$Gini(D)$​表示集合D的不确定性，基尼指数$Gini(D,A)$表示经$A＝a$分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。

基于此，其算法为：

1. 设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值a，根据样本点对$A＝a$​​的测试为“是”或“否”将D分割成$D_1$​​和$D_2$​​两部分，并计算$A＝a$​时的基尼指数。

2. 在所有可能的特征A以及它们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。

3. 对两个子结点递归地调用1，2步骤直至满足停止条件。

4. 生成CART决策树

算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。

#### 剪枝

剪枝算法的基本思想与基本的剪枝基本一致，但是：**对于固定的a，一定存在使损失函数$C_a(T)$​最小的子树，将其表示为$T_a$​。$T_a$​在损失函数$C_a(T)$​最小的意义下是最优的。容易验证这样的最优子树是唯一的。当a大的时候，最优子树$T_a$​偏小；当a小的时候，最优子树$T_a$​偏大。**

若使用递归的算法进行剪枝，可以证明，将a从小增大，可以产生一系列的区间$[a_i,a_{i+1})$​​，剪枝得到的子树序列对应着区间$a\in [a_i,a_{i+1})，i＝0,1,…,n$的最优子树序列${T_0，T_1,…,T_n}$的序列中的子树是嵌套的。

具体地, 从整体树 $\mathrm{T}_{0}$ 开始剪枝。对 $\mathrm{T}_{0}$ 的任意内部结点, $\mathrm{t}$, 以 $\mathrm{t}$ 为单结点树的损失函数是
$$
C_{\alpha}(t)=C(t)+\alpha
$$
以$t$为根结点的子树 $\mathrm{T}_{1}$ 的损失函数是
$$
C_{\alpha}\left(T_{t}\right)=C\left(T_{t}\right)+\alpha\left|T_{t}\right|
$$
当 $\mathrm{a}=0$ 及a充分小时, 有不等式
$$
C_{\alpha}\left(T_{t}\right)<C_{\alpha}(t)
$$
当a增大时, 在某一a有
$$
C_{\alpha}\left(T_{t}\right)=C_{\alpha}(t)
$$
当a继续增大，不等式反向，只要$a=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}$，$T_t$与$t$具有相同的损失函数，而$t$的节点更少，因此为较优方法，完成这一剪枝操作。

 为此, 对 $\mathrm{T}_{0}$ 中每一内部结点$t$, 计算
$$
g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
$$
示剪枝后整体损失函数减少的程度。在 $\mathrm{T}_{0}$​ 中剪去 $\mathrm{g}(\mathrm{t})$​ 最小的 $\mathrm{T}_{\mathrm{t}}$​, 将得到的子树作为 $\mathrm{T}_{1}$​, 同时将最小的 $\mathrm{g}\left(\mathrm{t})\right.$​ 设为$\alpha_1$，而$T_1$ 为区间 $\left[\mathrm{a}_{1}, \mathrm{a}_{2}\right)$​​的最优子树。 如此剪枝下去, 直至得到根结点。在这一过程中, 不断地增加a的值, 产生新的区间。

简而言之，即是设定某一$\alpha$​​后，对所有节点逐个进行计算与剪枝，最终，针对所有可能的子树，筛选出一个最优的子树以及其对应的参数。

##### 算法

输入: CART算法生成的决策树 $\mathrm{T}_{0}$​; 输出：最优决策树 $\mathrm{T}_{\mathrm{a}^{\circ}}$​
(1) 设k $=0, \mathrm{~T}=\mathrm{T}_{0}$​ 。
(2) 设a $=+\infty$​ 。
(3) 自下而上地对各内部结点, 计算 $\mathrm{C}\left(\mathrm{T}_{\mathrm{D}}\right),| \mathrm{T}_{\mathrm{t}|}$​ 以及
$$
\begin{gathered}
g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1} \\
\alpha=\min (\alpha, g(t))
\end{gathered}
$$
这里, $\mathrm{T}_{\mathrm{t}}$​ 表示以t为根结点的子树, $\mathrm{C}\left(\mathrm{T}_{t}\right)$​ 是对训练数据的预测误差, $|\mathrm{T}_{\mathrm{t}}|$​ 是 $\mathrm{T}_{\mathrm{t}}$​ 的叶结点个数。

(4) 自上而下地访问内部结点 $\mathrm{t}$​​, 如果有 $\mathrm{g}(\mathrm{t})=\mathrm{a}$​​, 进行剪枝, 并对叶结点$t$​以多数表决法决定其类, 得到树 $\mathrm{T}$​​ 。
(5) 设$k=\mathrm{k}+1, \mathrm{a}_{\mathrm{k}}=\mathrm{a}, \mathrm{T}_{\mathrm{k}}=\mathrm{T}$​​
(6) 如果 $\mathrm{T}$​​ 不是由根结点单独构成的树, 则回到步骤（4）
(7) 采用交叉验证法在子树序列 $\mathrm{T}_{0}, \mathrm{~T}_{1}, \ldots, \mathrm{T}_{\mathrm{n}}$​​ 中选取最优子树 $\mathrm{T}_{\mathrm{a}}$​​

### 数据处理

##### 连续数据

连续数据的基本处理方法是将其中位数作为划分点，完成二分类操作，但是某一属性可以被多次选择。

##### 缺失值处理

缺失值可以选择直接忽略该样本进行计算，但是在这一情况下，会损失大量数据，其解决方法是将同一个样本以不同的概率划分到不同的节点中去。在这一情况下，对信息增益函数做出改进：
$$
g(D,a)= \rho \times G(\tilde D,a)
$$
其中$\tilde D$即是该属性无缺失样本的集合。​

### 实例

```python
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

# Loading some example data
iris = datasets.load_iris()
X = iris.data[:, [0, 2]]
y = iris.target
# Training classifiers
clf1 = DecisionTreeClassifier(max_depth=4)
clf2 = KNeighborsClassifier(n_neighbors=7)
clf3 = SVC(gamma=.1, kernel='rbf', probability=True)
eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),
                                    ('svc', clf3)],
                        voting='soft', weights=[2, 1, 2])

clf1.fit(X, y)
clf2.fit(X, y)
clf3.fit(X, y)
eclf.fit(X, y)

# Plotting decision regions
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))

for idx, clf, tt in zip(product([0, 1], [0, 1]),
                        [clf1, clf2, clf3, eclf],
                        ['Decision Tree (depth=4)', 'KNN (k=7)',
                         'Kernel SVM', 'Soft Voting']):

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)
    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y,
                                  s=20, edgecolor='k')
    axarr[idx[0], idx[1]].set_title(tt)

plt.show()
```



[sklearn parameters](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)

